# WEEKLY人工無脳【第18号】（2018.11.5~11.11）



## ① Googleが圧倒的な自然言語処理手法を発表。東ロボくんの仇はGoogleが討つのか？

機械学習クラスター周りで今週最も話題になってたのはこれでしょうか。

https://togetter.com/li/1285134

Googleが公開した自然言語処理手法（新たな汎用言語表現モデル）が、これまでの自然言語解釈タスクでぶっちぎりの精度を出したという話。すごくざっくりいうと、機械による文章読解精度が劇的に上がったということ。

人工知能が昨今こんなに注目されている理由の根っこには、『「画像処理タスク」において人間のような高い精度を出せるようになったから』というところが始まりだったりします。2012年の画像解析コンペにおいて初めて深層学習手法であるCNNが使われ、二位にダントツの差をつけて優勝したあたりから社会的にも「AI（≒深層学習）がなんか凄いことになってるぞ」という雰囲気がでてきた感じでしょうか。その後も研究は続き、それからたった5年ほどで、今となっては人よりも高い精度で画像処理タスク（画像に何が映っているか判断する・不良品の検知など）を行えるようになっています。（※実際に人間の精度を超えたのは2015年）

ただ、それは「画像処理タスク」における大きな成功であって、実は「自然言語（人間の使っている言葉の）処理タスク」はまだそこまで大きな成功がありませんでした。自然言語処理タスクには、文章の解釈・翻訳・文章要約・文書生成などが含まれますが、確かに、google翻訳のような機械翻訳もかなり精度は上がりましたが、まだまだヘンテコな処理を行う例も目立ちます。文書要約や生成に至っては全然まだまだです。自然処理学会でも「実用的な翻訳システムはいつ頃できるか？」という質問に、「この5~10年くらいでできそう、という回答をもう何年も言ってます」というギャグがあるくらい、できそうでできない意外に難しい分野が「自然言語処理」です。



自然言語の機械的な解釈がなぜ難しいかという説明は、以下を見てもらうと一目瞭然かと。これは特に日本語独特の難しさだったりもしますが、他の言語でもそれぞれ難しさがあります。

https://twitter.com/nkmr_aki/status/1030799586737020930

文章にはいちおう文法というルールはあるものの、読み書きに数学のような明確な正解がないので難しい...



そんなわけで、2011年から始まったセンター試験の問題（文書）を解読して回答する[東ロボくんのプロジェクト](https://21robot.org/index.html)も、こういった難しさの前に成功できずに2016年にプロジェクトは終了しました。（ただし、2014年には、全国センター模試で全国の私大の８０％以上に合格可能というＡ判定を獲得するまでには成功してます。十分すごい。）



そこに一石を投じたのが今回のgoogleのBERTと呼ばれる新しい資源言語処理方法。@_Ryobotさんなど、有識者による解説ツイートは[こちら](https://togetter.com/li/1285134)にまとめられていますが、ざっくり言うと、これまでの自然言語処理の進化から、それこそ初めてCNNが投入された2012年の画像分析コンペのときのような非連続的な精度アップが報告されました。

汎用的な手法のため特に精度的なトレードオフもないらしく、さまざまな言語の自然言語処理タスクの性能を上げてくれそうということですごく注目されています。近々、「BERTで実装し直したらこんなに〇〇が向上した」という日本語記事もポロポロと出てくると思われます。

さすがに計算コストはかなり高く、googleお手製の計算チップTPUを使って4日間、GPUだと40~70日かかるようなものらしいのですが、googleはこの学習済みモデルを公開してくれています（日本語はない）

[こちらの記事](https://qiita.com//Kosuke-Szk/items/4b74b5cce84f423b7125)ではBERTを日本語に適応した話や、BERTの背景的なところも詳細に書かれていて勉強になります。

ところで、BERTの公開と同じタイミングで、東ロボプロジェクトから[センター試験の学習データが公開](https://21robot.org/dataset.html)されました。
そもそもセンター試験は教育指導要領の変更によって廃止され、2020年からは全く別の試験内容になるためこれを使って遊ぶにはちょっとモチベーションがあがりませんが、BERTに適応するとどうなるのかというのは気になるところです...



# ② 血液から14種のガン診断を目指す。PFNがやるなら勝確でしょ。

https://www.preferred-networks.jp/ja/news/pr20181029

PFDeNA（PFNとDeNAの合弁会社）が、血液から胃がんや肺がん、乳がんなど14種のがんを早期発見できる深層学習システムの開発を2021年の事業化を目指して始めるよという話。

がんは早期発見が重要なものの、受診コストや身体的負担のために検査受診率は3割ほどの低水準だそうです。血液検査するだけで早期発見できるのならこれほど便利なことはない。

> 近年の研究で、がんになると体液に含まれる「マイクロRNA」という物質の種類や量が変動することや、罹患した臓器によってマイクロRNAの発現に違いがあることなどが分かっている。開発するシステムでは、採取が容易な血液からマイクロRNAを計測。計測結果と臨床情報を使ってディープラーニングを行い、14種類のがんの有無を高精度で判定できるようにする。

深層学習の雄、PFNさんが絡んでいると勝利感のある夢のある話。実用化がんばってほしい。



# ③ 定期的にバスるけど一向に進化しないBabyTechのもどかしさ

https://twitter.com/hatarakedo1988/status/1059066159675273216

WEEKLY人工無脳でも何度もとりあげたことのあるBabyTech系のお話。
アプリでお母さんの睡眠時間や授乳回数を可視化したらめっちゃ大変だということが一目瞭然でわかった！という話。[togetter](https://togetter.com/li/1285040)にもまとめられます。

ここで紹介されている「ぴよログ」というアプリを実際にDLして触ってみましたが、授乳タイミングや授乳時間はともかく、睡眠時間まで手動入力しないけないのはやはりかなりめんどくさい。

例えば、Fitbitのようなライフロガーデバイスを手首に付けておくと、アプリに明示的に入力することなく、かなりの高精度で寝た時間・起きた時間を自動でアプリに記録してくれます。なので、お母さんに付けてもらって、お母さんが夜中に起きた時間≒赤ちゃんがおきて授乳を求めた時間 とすれば大体の行動ログをほぼ自動で記録することができて便利だと思うのです。

昔、そういったBabyTechのサービスを作ろうと赤ちゃんが生まれた母親にいろいろアンケートやヒアリングをしてみたことがあるのですが、結局の所、お母さんたちがそういった「手首に装着するデバイス」を嫌がっていたという印象でした。理由は、「子供に当たると危ないから付けたくない」「寝るときにも時計状のものをつけたくない」といったものが大半でした。前者は納得できますが、後者はがんばってほしいところ。

結局、こういった可視化を如何に「安全に」「ストレスフリー」に「自動で」できるかが肝だと思っていて、多くの人がこういった可視化ができれば育児は爆発的に便利になると思います。

（例えば、母親の激少な睡眠時間をリアルに可視化されれば流石に非協力的な父親でも何かしら対応するでしょう。また、夜中に起こされたお母さんたちをコミュニティー的に繋げて「今、日本中で夜泣きで起こされたお母さんは◯百人もいます、あなただけじゃないよ」という同期が取れれば育児ストレスも少しは緩和されるのではと思ってます）

データサイエンス的なアプローチで育児はやっぱりもっとアップデートできる。とても興味あるしお手伝いしてみたい分野です。誰か情報知ってたら教えてください！



# ④ 中国でAIニュースキャスター登場

https://jp.techcrunch.com/2018/11/09/engadget-ai-2/

中国国営ニュースメディアの新華社通信が、中国語と英語それぞれを話すAIニュースキャスター2名をデビューさせたという話。

> リアルな外観とそれほどでもない喋りに視聴者ザワつく

のとおり、実在人物をモデルとした見た目はリアルなものの、喋っている音声には電子的な音の影響を感じます。それでも国営メディアが実戦投入してやり始めるのはすごいですね、さすが中国。

https://youtu.be/GAfiATTQufk



[WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)や[DeepFake](https://www.youtube.com/watch?v=cQ54GDm1eL0)、[GoogleDuplex](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html)のように限りなく自然に話す仕組み自体はあるのでAIキャスターたちもきっとすぐ自然に話せるようになると思います。見た目ももう美少女VTuberでええんちゃうんかというのは置いといて。



# ⑤ 超正確な位置情報を「3単語」で表せることは実はすごいイノベーションかもしれない...

https://jp.techcrunch.com/2018/11/08/engadget-what3words-57-3/

ずっと注目しているwhat3words。世界中を3m四方に区切り、3単語で超正確な位置情報を表現する技術（アイデア？）
住所の曖昧さを解消できるので、「自動車・モビリティ・ナビゲーション」「配送・物流・Eコマース」「郵便サービス」「国家インフラ」「無人航空機」関連企業からおそろしい出資額を受けて注目されています。

今回の記事中には日本の位置を3単語で表している画像も付いててテンションあがった。

https://jp.techcrunch.com/2018/11/08/engadget-what3words-57-3/



Sonyが出資した理由に、

> Sony Innovation Fundは今回の出資の理由について「what3wordsが機械に正確な位置情報を音声で入力するという大きな課題を解決してくれました」と語ります。

と書かれていてなるほどなーと。

カーナビアプリにも音声で行き先を指定するUIはあるものの、住所が長すぎたりすると認識失敗することがよくあります。それに対して「3つの単語」を言うだけなら確かに音声識別の精度はほぼ100%になりそう。what3wordsのアイデア自体もコロンブスの卵だし、それを活用しようとする人たちのアイデアもまた面白い。



# ⑥ 「理解できる」はどこまで重要なのか、もしくはいつ手放すのか　

https://wired.jp/2018/11/08/google-ai-tool-tumors-mutations/

医療系のAIでは、「なぜその領域をガンと判断したか」というように、判断根拠をブラックボックスにすることなく説明できる能力（機能）が強く求められています。しかし、人間には取り出しきれない（理解しきれない）情報を画像データは確かに持っていて、AIならそれがわかるかもということもまた事実。「根拠はわからないが99%正しい答えを返すAI」を我々はどう扱えばよいのかという話。

googleの物体検出アルゴリズムInceptionV3を転移学習し、腺がんと扁平上皮がんの2種類の肺がんの区別を学習させるときに、それぞれの腫瘍の遺伝子変動情報も与えると識別精度が上がったという話。ただし、この精度向上の根拠は不明であるらしい。

これに対して、「根拠不明だから利用できない」とするのか、「根拠不明であっても99%正しい答えをだすアルゴリズムを利用しないのは馬鹿げている」とするのかで医師の間でも意見が割れるらしい。

> 「本当の新規性は、AIが人間と同じくらい優れていることを示したことではなく、人間の専門家ができない識別が可能であることを示唆した点でしょう」

おそらくこの先も、「人間の専門階が出来ない識別（そして根拠も理解できない）が、AIはできる」という事例が医療分野以外にもたくさん出てくるはずです。そうした時に、人間が諦めて盲目的に従うか、謎AIの使用を厳密に制限するかはまた難しい話になりそうです。



# ⑦ GANで顔の悪魔生成

https://twitter.com/darren_cullen/status/1060225126313156613?s=21

Progressive Growing of GANsを使って50,000の画像を学習して生成した顔動画。怖くて面白かったので。





# 与太話

最近母校の高校で、文理選択や職業について考える総合学習の時間（？）で自分の仕事であるデータサイエンティストについてのお話をしてきました。スライド公開もしているので良かったら見てみてください。
個人的にはデータサイエンティストの話だけではなく、今AIが社会をどのように変えていこうとしているかという話をしたかったのでそれについても書いてみました。

https://speakerdeck.com/ysdyt/zhi-ye-shao-jie-detasaienteisuto



そして、有志で開催しているデータ系の勉強会の第四回目を公開中です。

https://brainpad-meetup.connpass.com/event/107513/



火曜日まで応募受付中なのでデータ系の方ぜひご応募ください！コンテンツは、

- 位置情報データのマーケティング活用

- 技術書典のサークル座席配置最適化

- ブレインパッド社 部長によるデータサイエンティスト部マネジメントについて

  の結構濃度が濃いめの３本です。

