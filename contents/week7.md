# WEEKLY人工無脳【第7号】（2018.5.14~5.20）

![eye_catch](./../figs/weekly_jinkoumunou.png)

## ① AI開発を受託会社に発注する際の心構え、「”AIガチャ”の回し方」がウケるw（が、ワロエナイ）

今週最も笑った記事。AI業界にいる人はニヤニヤすることでしょう。

https://note.mu/maskedanl/n/n35fa7bb528c0

https://twitter.com/maskedanl/status/996228136726478848

世の中の「AI受託開発会社」の技術ステータスや、実際に発注すると何が起こるのかの「あるある」話を面白おかしくまとめてくれてます。ただし、面白おかしいのは筆者による文才であって、内容自体は笑えないかも…

紹介されているのは、大手SIer/外資系企業/有名なAI開発会社/独立系SIer/フリーランス/クラウドソーシング/自称AIベンチャー の7種。内容は…まぁたしかに一般的に聞く評判だとーと感じました。世はまさにAI開発戦国時代。

最後のまとめ部分の1文は紛れもない真実ですので、AI開発を他者に発注する人は心に留めておきましょう。
>少なくとも「ここに頼めば大丈夫」と保証できる会社（ガチャ）はありません。そもそもAI開発自体が成功を保証できないので、一定の損失は覚悟しましょう。

## ② 機械学習工学研究会のkickoffが超人気でスゴイ（語彙力）

上の話から微妙に続くのですが、
AI開発（機械学習で動く何か）がなぜ難しいかというと、AIシステム（と呼ばれている何か）は内部で状況（インプットされるデータ）ごとに異なる計算/答えを返す機械学習を行うので、従来のソフトウェア工学のアプローチによるシステム構築とは違う難しさがあるからです。それに立ち向かう研究会のkickoffが今週行われました。難しそうな研究会なのに500人規模の超満員だったようです。

https://sig-mlse.wixsite.com/kickoff

登壇者の方のスライドも公開されています

https://mlxse.connpass.com/presentation/

機械学習システムの難しさについては、[久保さん](https://www.slideshare.net/takahirokubo7792/2018-97367311)や[有賀さん](https://www.slideshare.net/chezou/ss-97373367)のスライドが詳しいです。

ツイッターのハッシュタグでもこの分野のツラミを分かち合う話題などいろいろと盛り上がりをみせ…

https://twitter.com/yh_taguchi/status/996987693258059776?s=21

初回ということもあり、イマイチどういった会なのかがわからないまま参加された方も多かったようです。
当日1番多かった質問は、「資料が配布されるかどうか」だったという風の噂も聞きました… これから賑わいを見せそうな集まりで期待です。


## ③『素晴らしい瞬間』を自動的に撮影するGoogleのカメラは膨大な人力作業で作られていた…

https://ai.googleblog.com/2018/05/automatic-photography-with-google-clips.html

個人的にめちゃくちゃ関心の高いカメラの話です。
[”Google Clips”](https://store.google.com/us/product/google_clips?hl=en-US)は胸元に付けられるほど小さなカメラで、「良い感じの瞬間」を自動で認識しショート動画を撮影してくれるカメラ。USでは＄249で発売されている模様。
機械的に「良い感じの瞬間」をどうやって判断しているのかがもちろん気になるところですが、その技術詳細についてGoogle AI blogが出ていました。

箇条書きでざっくりと中身を書くと、

- clipsの3つの重要な原則
  - 処理は全てエッジ（カメラ）で行っている
  - 魅力的な瞬間にビデオを撮影する（静止画は撮っていない）
  - 撮影対象は人物や動物であり、風景や景色などは対象となっていない

- 「素晴らしい瞬間」の認識のためのデータセット作成
  - 1,000本以上の動画からランダムにショート動画を作成し、ランダムにペアを作る。
  - 「2つのショート動画のうち、どちらが好み？」という評価をプロの写真家やビデオ編集者につけてもらう。
  - 5千万組(！)のペア比較を実施し、動画にスコアを付与。

- 学習
  - 動画に映る物体や動作の検知を行い、評価者が付けたスコアと相関が高い物体ラベルを絞り込む
    - 笑顔/ペット/ハグ/キス/ジャンプ/ダンス など一般的に好まれそうなラベルにはスコアに重みを付ける
  - スコアと物体の関係性を深層学習を使い計算能力の高いサーバーで学習
  - 実際の推論はエッジで行うため、MobileNet Image Content Model（ICM）という小さなモデルで上記のモデルの推論結果を模倣するモデルを作成。関連性の低いコンテンツを無視しながら、写真の最も興味深い要素を認識するようにする。
  - こうして、動画を見せるとそのスコアを推定するモデルを作成

- その他いろいろ
  - バッテリーは3時間持つ。デバイスは常に起動しており、1秒間に1フレームをキャプチャする低電力モードでバッテリーの大半を費やす
  - 現在のフレームのスコアが、直近で撮影された動画によって設定されたしきい値を超えると、高出力モードに移行し15fpsでキャプチャする
  - 様々な性別や人種に偏ったデータセット・評価結果にならないように慎重にテストはしているが、機械学習の公平性は重要なポイントなので今後も検討していく
  - 結果として、単純に物体検出を行っているだけではなく、写真に対する「人間の好み」も反映した「素晴らしい瞬間」を撮影するアルゴリズムが出来たと思っている。

技術的な設計などはもちろんですが、ヤバイのは5千万の動画ペアに評価ラベルを付けたという件… googleパワーはこういった泥臭い作業も（金と外注の力を使っていても）完遂することろかと。


## ④ 人類と深層学習の戦争はすでに始まっている、最初の犠牲者はジロリアンか？

GANでラーメン二郎生成… 凄すぎないかコレ… でもジロリアンならGANよりも優秀なdiscriminatorとして見抜けるんだよね…？

https://twitter.com/knjcode/status/997458395987828736?s=21


## ⑤ 「データはユーザの声なき声」

今週は「Data Analyst Leaders Talk!」というイベントがあったようです（申込んだけど落ちた勢）

https://techplay.jp/event/668507

>DeNA、グリー、メルカリ、楽天の4社それぞれの分析チームのマネージャ・執行役員が、自社のデータ分析の活用と組織について、パネルトーク形式で切り込んでお話をします。

みたらわかる、これ絶対おもしろいやつやん。

案の定、ツイッタを見ていると非常に好評だったそうで、さっそく幾つかブログも上がってました。中でも[「データはユーザの声なき声」](https://note.mu/kahonyun/n/nc6d6421022c4)というのはまさにそうだなーと。分析でそういったインサイトを見つけられると超かっこいいですね。

余談ですが、ユーザーのインサイトといえば、[誰でも無料でもらえる「離婚届」の用紙がメルカリでジャンジャン売れているらしい](https://togetter.com/li/1103915)という話が最近面白いなーと思いまして、つまり、ユーザーの

> 「役所でご近所さんに見られるのマズイけど、手元に持っておきたいので600円くらいなら買うわ需要」

にハマったということらしいです。まさに声なき声。
メルカリで[謎のアイテムが売れてる](https://toyokeizai.net/articles/-/169214)ときは、ユーザー達の面白いインサイトが見つかる瞬間でもあるのでなんだか分析が楽しそうです。


## ⑥ 実は人間はもっと多くのものを「視ている」らしい。しかも無意識に。

https://shiropen.com/seamless/spatial-representations-of-the-viewers-surrounding

東北大学電気通信研究所の研究結果で、人間の視覚系が無意識に周囲環境を学習し脳内にモデルをつくることで、直接見ることができない頭の後ろの情報も処理していることを明らかにしました、らしい。（何を言っているかわらないと思うが…自分もよくわかってないんだ…）

実施された実験は、被験者の周りを360度囲うように6枚のディスプレイを設置し、その6つのディスプレイ上に1つの標的（「T」）および35の引っ掛けの標的（「L」）が提示され、被験者は「T」だけを探し、発見までの時間を記録するというもの。（たぶん後ろを振り向いて確認したりしてはいけない。）
文字の配置が毎回新しいものに変わる「ランダム配置」と、12の配置パターンを繰り返す「繰り返し配置」を何百回も行い比較した結果、「繰り返し配置」の場合は「ランダム配置」と比較し、有意に発見時間が短かく、しかもそれが例え背面のディスプレイにTが出現した場合でも統計的有意に短かったそうです。
これは、正面にあるものから、背後にあるターゲット位置を推測することができる能力といえ、しかもこのことに被験者は全く気付かないことから、潜在学習によって周囲の環境を理解しているといえるらしい。

つまり、人間の視覚的知覚には視覚的入力以上のものがあり、身体や文脈情報の影響が示されていると論文には報告されています。人間はただ見えているものを認識したり学習しているだけではなく、自分の意識外でもパターン学習が行われているらしい、とのこと。脳科学的な興味は、人間が意識外で獲得したこういった情報は脳のどこに保存されてるのかということらしい。

「人間は視覚から意識外の情報を取得している」という話は、このブログの第2号でも紹介した[「人は相手の顔の血色の変化だけで感情を読み取れる」という話](http://ysdyt.hatenablog.jp/entry/week2)と同じような話なのかなと思い、かなり興味深いです。人間の認知はきっともっと広いんでしょうね。ワクワク。

## ⑦ 「未分類のゴミ」を貯める前にデータ分析官を雇おう（←それな！！！）

https://tjo.hatenablog.com/entry/2018/05/15/193000

明確なルールや基準もなく無差別にデータを集めても「未分類のゴミの山」ができるだけで、きちんと目的と設計を立てられるデータサイエンティストが居て初めて、データを集める前から「可燃」「不燃」「資源」のようにゴミの中から使えそうなデータをより分けることができ、使いものになるものも見つけられるかもね、というお話。

アルゴリズムを考えたり分析することと同じように、「そもそもどういったデータをどういう形式でどれくらい集めるか」という判断は、分析官の中でもわりと経験がないと難しい。それなのに分析官不在の状況で、「とりあえずデータは貯めとこう」とすると返って、データ取得の手戻りを増やしたり無駄な前処理をする必要に迫られたりして良くない。データ活用をしたいなら、データ集めをする”前”に分析官を集めましょう（相談しましょう）という話。


## ⑧ 商品倉庫のピックアップロボットはAmazonだけじゃないぞ

https://gigazine.net/news/20180514-ocado-robot-warehouse/

Amazonの出荷倉庫で[ロボットが商品をピックアップしている](https://gigazine.net/news/20150819-amazon-order-tour/)話は有名ですが、イギリスの大手オンライン小売業者「Ocado」でも可愛いロボットが働いているんだぜ、という話。個人的にはAmazonロボットよりもデザインがかわいくて好き。

25万個のマス中に商品が配置されており、数千台のロボットが秒速4mのスピードでマス上を動き回る。1秒間に300万回もの計算を行って最適かつ衝突の起こらない稼働を実現しているそうです。ロボット1台が運べる重量は10kg。充電が切れそうになると勝手にスタンドに入ってきて充電してまた仕事に戻る（かわいい）

こういう自動化はもっともっと進化してほしいですね。
