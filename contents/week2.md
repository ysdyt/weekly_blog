# WEEKLY人工無能【第2号】（2018.4.9~4.15）

![eye_catch](./../figs/weekly_jinkoumunou.png)

## 人は相手の顔の血色の変化だけで感情を読み取れるという衝撃

[http://www.pnas.org/content/early/2018/03/16/1716084115:title]

毎週聞いてるpoadcast「バイリンガルニュース」の#308（34分頃から）で紹介されていた面白い話。人は相手の**顔の血色の変化だけで相手の感情を読み取れている**という研究について。

人は怒ると鼻や頬の血色が変化しそれは年齢や人種を問わず同じパターンが見られる。研究チームは、18種類の感情で見られる血色の変化を無表情のヒトの顔写真に重ね、感情を推定させるテストを20人の被験者に対して行ったところ、それぞれの感情を幸せ・悲しみ・怒りの表情を約7割ほどの確率で当てることができた。（つまり、**筋肉の動きが無くても、血色だけで感情が読み取れていた**）。また、喜んでいる人の顔に怒りの血色を載せて感情をmixすると、被験者は「理由は不明だが違和感がある」ということまで気づいた。さらに、同じことを人工知能にさせると幸せ・悲しみ・怒りの表情を約9割の確率で当て、人間より高い精度を示した、という内容。

顔には他の部位には見られないほど皮膚の表面近くに血管が密集しているらしく、この話ともリンクしているのかもしれない。「おフェロメイク」のようなチーク濃いめの血色をよく見せるメイク方法も実はこういった心理に効いていたりするのならば非常に面白い。

人工知能（おそらく比較的単純な画像の分類問題を解く深層学習モデルだと思われる）がこういったタスクで人よりも高い精度を出すのもうなずける。というよりまさに機械が得意なタスクじゃん。今後に期待。

この研究結果が真実で、人間は実は無意識のうちに処理している情報が他にもまだまだあったりすると面白いなー

P.S  [こちらの先生](http://www2.ece.ohio-state.edu/~aleix/)の研究らしい 。この技術を活用して[online-emotion](http://online-emotion.com/)という企業も始めたそう。オハイオ大学のリリース記事は[こちら](https://news.osu.edu/news/2018/03/19/firstblush/)

## 誰でもディープラーニング出来る時代に難しいのは「現状のビジネスにどうやってイイ感じにfitさせるのか。開発コスト分ペイするのか」問題

[http://www.itmedia.co.jp/news/articles/1804/04/news118.html:embed:cite]

AIと言う名の画像分類とチャットbotを福岡のクリーニング店 店長が独学で作って活用しているよという話。

衣類を精度良く24クラス分類したり、ITリテラシーの低い店員にも使えるようにチャットbotのUIを工夫するなど独学で進めたものの、「で、これどうやって業務に活用しよう？」みたいな話。
AI開発上の問題となった点としてあげられている

>（1）課題設定が明確でないと、自分の目的に合ったデータを使っての機械学習ができない
>（4）実際の業務オペレーションや費用対効果がどうなるか分からない

という点はまさに「深層学習モデルはひとまず誰でも動かせるようになったけど、で、これどう役に立つの？」アルアルで、データ分析の話をしていたはずなのに、気づいたらコンサルみたいな仕事しかしていない、みたいな感じになる。技術だけが異様に早く進化した結果として今現在、至る所で起こっている話だと思われる。

それにしても、キュウリの人といい、Google Japnaはこういう人みつけてくるのうまいな。AIで監視しているのだろうか。

## 深層学習を使った新しいアート手法が生まれそうで面白い

[https://www.gizmodo.jp/2018/03/learning-see.html:embed:cite]

[https://twitter.com/quasimondo/status/982711735001010176?ref_src=twsrc%5Etfw&ref_url=https%3A%2F%2Fnote.mu%2Fysdyt%2Fm%2Fme6d7660e5c6f&tfw_site=note_PR:embed]

最近、twitterなどでこういう系のアート（？）が流れてきて面白い。
原理が詳しく説明されていなくて謎なんですけど、おそらく特定対象物の画像（炎・花・海・人の顔など）だけを学習させたauto encoder系の何か？それをリアルタイムでpredictさせているのだろうか？そうだとしても思った以上にうまく描写されていて素敵。デジタルアート的な文脈で発展するだろうか。

## 実行パラメータの数字をファイル名にいれたりして学習結果を管理している人に朗報

[https://jp.techcrunch.com/2018/04/06/2018-04-05-cometml-wants-to-do-for-machine-learning-what-github-did-for-code/:embed:cite]
[https://twitter.com/icoxfog417/status/984062320971141121?ref_src=twsrc%5Etfw&ref_url=https%3A%2F%2Fnote.mu%2Fysdyt%2Fm%2Fme6d7660e5c6f&tfw_site=note_PR:embed]
[https://medium.com/liaro-engineering-blog/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AEgithub-cometml%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F-eed920be46c9:embed:cite]



実際に深層学習モデルを学習させている人はわかってくれると思うが、モデル精度を上げるために行うパラメータ調整はけっこうな組み合わせでパラメータ変更を行うので、学習に実行したパラメータとその学習結果を記録するのが非常に煩雑になり困る。ファイル名やディレクトリ名にパラメータ数を書き込む、みたいなことをする俺俺便利ツール君みたいなものを作っている人も少なくないかもしれない。たぶんCometMLはそういうことを解決してくれる。事業会社で最近流行りの深層学習基盤も、自動で再学習を行った場合にモデルファイルの管理などをどうするかという問題があると思うが、そういった点も何かしらうまいベストプラクティスを提供してくれるのかもしれないし、してくれないかもしれない。

## 「六次の隔たり」の次元数はますます減っている

[https://www.gizmodo.jp/2018/04/311thousand-fb-accounts-leaked-from-just-53.html:embed:cite]

先週も書いたCambridge Analyticaの続報。オーストラリアではたった53人のユーザーから31万人分のデータが流出していたという話。

> 問題のアプリをダウンロードしていなくても、繋がっている友人によってはデータが流失してしまう。｢繋がる｣ことにサービス最大の意味を持つFacebookだけに、なんとも恐ろしいことです

「六次の隔たり」みたいな話を地で行っている。漏れたデータはもうどうにもならないが、この事件がどう収められるのかだけ、私 気になります。ザッカーバーグさんもずっと詰められててかわいそう。

## "オープンコミュニケーション"を推進するアイデア

[http://mercan.mercari.com/entry/2018/04/10/163322:embed:cite]
[https://twitter.com/hbkr/status/984640857037914112:embed]

「slackサイコー！便利ー！yeeeeah！」って感じのメルカリさんのslack事例記事。内容的には「なにをいまさら...」感がすごいあるものの、めちゃくちゃシェアされている。

ただ、「オープンコミュニケーション文化を推進するためにDM率などを監視する」というようなアイデアは面白い。メールを脱した21世紀の人類のやり方だなーと感じる。slackが便利だということは多くの人が既に知っているが、slackが提供するメッセージやりとりの統計情報の価値を理解している人はまだ少なそう。

## （ニッチだけど）身近なものを対象に深層学習使う話は心が踊るよね

[https://qiita.com/IshitaTakeshi/items/915de731d8081e711ae5:embed:cite]

道路のひび割れを物体検知でリアルタイム推定する話。東大の研究室らしく、arXivに[論文](https://arxiv.org/abs/1801.09454)も出てる。

VGG16ベースのSSDで精度良く予測ができているらしい。論文をちらっと読んでみると、車内に簡単にマウントしたスマフォで検出用の画像を取得するらしく、時速40kmで走行する車の前方10メートル先の画像を撮影して、その画像を1.5秒で推論し、ヒビの位置を返す。スマフォのような（超高精細ではない）お手軽なカメラを使うというのがよい。自動運転時の路面状態を判断するようなタスクで活躍するのだろうか。それにしても論文内容をQiitaにもまとめる時代なのかー。

2017年の慶応SFCの研究室公開イベントORFでも、機械学習系の研究室が「路面や道路標識の剥がれ」をSSDで検出している内容が発表されていた。データの収集は町中を走るゴミ収集車の車載カメラから提供してもらっていたという話だった気がする。アルゴリズムなどは無料で公開もされているから、こういった研究のキモはどうやって精度のよいデータを集めるか、分析設計をどうするか、アウトプットの具体的なイメージや実社会で実運用にも耐えうるか、みたいなところがポイントになるのだろうか。

## 深層学習で「カクテルパーティー効果」っぽい機能を再現

[https://shiropen.com/seamless-ai-google-looking-to-listen:embed:cite]

人間が雑音の中から特定の音だけを拾える「カクテルパーティー効果」は、人間（動物？）が行うユニークな認知の一つだったが、機械も同じような機能が行えるようになったらしい。ただ、この手法は

>本提案手法は、音声を分離するために入力ビデオの聴覚信号と視覚信号の両方からアプローチしています。人が話す声と口の動きは相関関係があるため、両者を捉えることで精度は向上します。

というかんじで、画像（口の動き）と音声の両方を使っているので、厳密には人間のカクテルパーティー効果ではないはず（人間は聴覚だけでそれが行える）でもそんなこまけーことはどうでもいい、人間と同じような機能をまた一つ獲得したのだから。

個人的には最近、音声系のネタが増えてきたように感じている。

## 医師の代わりに医療判断を行うAIが世界で初めて認可される。医療の流れが変わるぞ

[https://www.theverge.com/2018/4/11/17224984/artificial-intelligence-idxdr-fda-eye-disease-diabetic-rethinopathy:embed:cite]
[https://www.technologyreview.jp/nl/fda-approves-first-ai-powered-diagnostic-that-doesnt-need-a-doctors-help/:embed:cite]

医療系サービスでアメリカFDAの認可を受けるというのはかなりスゴイことで、googleセルゲイ・ブリンの奥さん（生物学者のアン・ウォイッキ氏）と他2名が起業した遺伝子解析サービス「23andMe」もその検査精度を理由にFDAからサービス中止勧告を受けたりしていた（2018年3月には[公式に認可が降りたらしい](https://jp.techcrunch.com/2018/03/07/2018-03-06-23andme-gets-fda-green-light-for-cancer-risk-test/)が。）

AIによる医療診断サービスがそんな厳しい認可を突破した事例が一つでも出来たということは、今後もこういったAI医療がたくさん出て来ると予想される。医療に限っては決して「医師が不要になる」ということはないだろうが、こういったサービスが医師の負担軽減となり、医療業界全体の風向きが良い方向に成ることを期待している。画像認識系のタスクで解ける医療判断はまさに深層学習が得意な分野。記事でも精度9割と書かれていたので、医師の判断精度と比較しても十分実戦投入出来る精度になったということなのだろう。

今後AIによる医療診断システムをFDAが認可していくということも中の人がツイッターで発言しているそうなので次のニュースにも期待。

## 次世代の脳波計？スゲーっぽいけど、なんで顎なんだぜ？

[https://jp.techcrunch.com/2018/04/07/2018-04-06-mits-new-headset-reads-the-words-in-your-head/:embed:cite]

明言はされていないが、おそらく脳波計の一種だと思われるが、記事では顎の神経筋から信号を読むとしているので、脳波ではないのかもしれない。頭などに設置して脳波を測定するデバイスは、脳からの物理距離が離れるほどノイズが乗ってしまうので、わざわざ顎に計測部分を設置する理由が謎すぎる。

MITチームがつくっているということと、確度が92%ということなのでとんでも製品ではないと思われるが、見た目のインパクト含め謎が多い。

ちなみに、日本では、阪大発ベンチャーのPGVなどが軽量・低価格な[脳波計デバイス](http://astavision.com/contents/interview/4368)を開発している。

## 当たり前のように機械学習APIをwebサービスに使う時代。機械学習をセンス良く使えるか？

[http://sakamoto2.hateblo.jp/entry/2018/04/13/190000:embed:cite]

Googleの[CLOUD NATURAL LANGUAGE](https://cloud.google.com/natural-language/) APIを使って、はてブホットエントリーのコメントを対象に感情分析して、感情ごとに記事を集約したキュレーションサービスを作った話。機械学習適応先を選択するセンスが良い。

行われている事自体は難しいことはないが、やはりこういったお手軽な機械学習APIは読者の関心が高いらしい。

半年ほど前に同APIが案件で利用できるかどうかいろいろ触ってみたときは、日本語に関して感情分析精度がかなり低かったので使用は見送ったが、ブログ中で試されている「チョコレートが好きすぎて死にそう」「チョコレート嫌いなので食べると死ぬ」は自信を持ってポジティブ・ネガティブを推定できるようになっているので良さそう。googleのことだからひっそりと精度を高めていっている可能性もある。

機械学習・深層学習を使ったwebサービスは一発芸感があるが、ビジネスにおいても機械学習をどのようにインパクトのある使い方が出来るかという設計やセンスは、それもまた「一発芸」的なところがあるのでバカにしてはいられない。


```
■「WEEKLY人工無能」は、筆者がSNSや日頃の雑談で知ったネタを独断と偏見でまとめているブログです。
■「WEEKLY人工無能」は、筆者がその話題を知ったタイミングでまとめているため、「記事公開自体は先月」といった可能性も十分にあり得ます。速報性よりも話題性を重視していることをご了承ください。
■「WEEKLY人工無能」は個人の余暇で運営しているため、調べが足りないこともあります。誤りがあれば優しく教えてください。
■「WEEKLY人工無能」は「独断ニュース（http://dokudan-weekly.hatenablog.jp/）」に刺激を受けて書き始めた、独断ニュースのデータサイエンス・人工知能業界版です。飽きるまで適当に続けます。
```